@misc{AB14,
  note = {A.~Agarwal and L.~Bottou. \newblock A lower bound for the optimization of finite sums. \newblock \emph{Arxiv preprint arXiv:1410.0723}, 2014.}
}

@misc{AO14,
  note = {Z.~Allen-Zhu and L.~Orecchia. \newblock Linear coupling: An ultimate unification of gradient and mirror descent. \newblock \emph{Arxiv preprint arXiv:1407.1537}, 2014.}
}

@misc{Ans98,
  note = {K.~M. Anstreicher. \newblock Towards a practical volumetric cutting plane method for convex programming. \newblock \emph{SIAM Journal on Optimization}, 9\penalty0 (1):\penalty0 190--206, 1998.}
}

@misc{ABM11,
  note = {J.Y Audibert, S.~Bubeck, and R.~Munos. \newblock Bandit view on noisy optimization. \newblock In S.~Sra, S.~Nowozin, and S.~Wright, editors, \emph{Optimization for Machine Learning}. MIT press, 2011.}
}

@misc{ABL14,
  note = {J.Y. Audibert, S.~Bubeck, and G.~Lugosi. \newblock Regret in online combinatorial optimization. \newblock \emph{Mathematics of Operations Research}, 39:\penalty0 31--45, 2014.}
}

@misc{Bac13,
  note = {F.~Bach. \newblock Learning with submodular functions: A convex optimization perspective. \newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 6\penalty0 (2-3):\penalty0 145--373, 2013.}
}

@misc{BM13,
  note = {F.~Bach and E.~Moulines. \newblock Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n). \newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}, 2013.}
}

@misc{BJMO12,
  note = {F.~Bach, R.~Jenatton, J.~Mairal, and G.~Obozinski. \newblock Optimization with sparsity-inducing penalties. \newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 4\penalty0 (1):\penalty0 1--106, 2012.}
}

@misc{Bar14,
  note = {B.~Barak. \newblock Sum of squares upper bounds, lower bounds, and open questions. \newblock Lecture Notes, 2014.}
}

@misc{BT03,
  note = {A.~Beck and M.~Teboulle. \newblock {M}irror {D}escent and nonlinear projected subgradient methods for convex optimization. \newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0 167--175, 2003.}
}

@misc{BT09,
  note = {A.~Beck and M.~Teboulle. \newblock A fast iterative shrinkage-thresholding algorithm for linear inverse problems. \newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (1):\penalty0 183--202, 2009.}
}

@misc{BN01,
  note = {A.~Ben-Tal and A.~Nemirovski. \newblock \emph{Lectures on modern convex optimization: analysis, algorithms, and engineering applications}. \newblock Society for Industrial and Applied Mathematics (SIAM), 2001.}
}

@misc{BerVem04,
  note = {D.~Bertsimas and S.~Vempala. \newblock Solving convex programs by random walks. \newblock \emph{Journal of the ACM}, 51:\penalty0 540--556, 2004.}
}

@misc{BV04,
  note = {S.~Boyd and L.~Vandenberghe. \newblock \emph{Convex Optimization}. \newblock Cambridge University Press, 2004.}
}

@misc{BPCPE11,
  note = {S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein. \newblock Distributed optimization and statistical learning via the alternating direction method of multipliers. \newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 3\penalty0 (1):\penalty0 1--122, 2011.}
}

@misc{Bub11,
  note = {S.~Bubeck. \newblock Introduction to online optimization. \newblock Lecture Notes, 2011.}
}

@misc{BC12,
  note = {S.~Bubeck and N.~Cesa-Bianchi. \newblock Regret analysis of stochastic and nonstochastic multi-armed bandit problems. \newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 5\penalty0 (1):\penalty0 1--122, 2012.}
}

@misc{BE14,
  note = {S.~Bubeck and R.~Eldan. \newblock The entropic barrier: a simple and optimal universal self-concordant barrier. \newblock \emph{Arxiv preprint arXiv:1412.1587}, 2014.}
}

@misc{BEL15,
  note = {S.~Bubeck, R.~Eldan, and J.~Lehec. \newblock Sampling from a log-concave distribution with projected langevin monte carlo. \newblock \emph{Arxiv preprint arXiv:1507.02564}, 2015{\natexlab{a}}.}
}

@misc{BLS15,
  note = {S.~Bubeck, Y.-T. Lee, and M.~Singh. \newblock A geometric alternative to nesterov's accelerated gradient descent. \newblock \emph{Arxiv preprint arXiv:1506.08187}, 2015{\natexlab{b}}.}
}

@misc{CR09,
  note = {E.~Cand{\`e}s and B.~Recht. \newblock Exact matrix completion via convex optimization. \newblock \emph{Foundations of Computational mathematics}, 9\penalty0 (6):\penalty0 717--772, 2009.}
}

@misc{Cau47,
  note = {A.~Cauchy. \newblock M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d'{\'e}quations simultan{\'e}es. \newblock \emph{Comp. Rend. Sci. Paris}, 25\penalty0 (1847):\penalty0 536--538, 1847.}
}

@misc{CL06,
  note = {N.~Cesa-Bianchi and G.~Lugosi. \newblock \emph{Prediction, Learning, and Games}. \newblock Cambridge University Press, 2006.}
}

@misc{CP11,
  note = {A.~Chambolle and T.~Pock. \newblock A first-order primal-dual algorithm for convex problems with applications to imaging. \newblock \emph{Journal of Mathematical Imaging and Vision}, 40\penalty0 (1):\penalty0 120--145, 2011.}
}

@misc{CHW12,
  note = {K.~Clarkson, E.~Hazan, and D.~Woodruff. \newblock Sublinear optimization for machine learning. \newblock \emph{Journal of the ACM}, 2012.}
}

@misc{CSV09,
  note = {A.~Conn, K.~Scheinberg, and L.~Vicente. \newblock \emph{Introduction to Derivative-Free Optimization}. \newblock Society for Industrial and Applied Mathematics (SIAM), 2009.}
}

@misc{Cov92,
  note = {T.~M. Cover. \newblock 1990 shannon lecture. \newblock \emph{IEEE information theory society newsletter}, 42\penalty0 (4), 1992.}
}

@misc{Asp08,
  note = {A.~d'Aspremont. \newblock Smooth optimization with approximate gradient. \newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0 1171--1183, 2008.}
}

@misc{DBLJ14,
  note = {A.~Defazio, F.~Bach, and S.~Lacoste-Julien. \newblock Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. \newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}, 2014.}
}

@misc{DGBSX12,
  note = {O.~Dekel, R.~Gilad-Bachrach, O.~Shamir, and L.~Xiao. \newblock Optimal distributed online prediction using mini-batches. \newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 165--202, 2012.}
}

@misc{DSSST10,
  note = {J.~Duchi, S.~Shalev-Shwartz, Y.~Singer, and A.~Tewari. \newblock Composite objective mirror descent. \newblock In \emph{Proceedings of the 23rd Annual Conference on Learning Theory (COLT)}, 2010.}
}

@misc{DH78,
  note = {J.~C. Dunn and S.~Harshbarger. \newblock Conditional gradient algorithms with open loop step size rules. \newblock \emph{Journal of Mathematical Analysis and Applications}, 62\penalty0 (2):\penalty0 432--444, 1978.}
}

@misc{FW56,
  note = {M.~Frank and P.~Wolfe. \newblock An algorithm for quadratic programming. \newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0 95--110, 1956.}
}

@misc{FT07,
  note = {M.~P. Friedlander and P.~Tseng. \newblock Exact regularization of convex programs. \newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (4):\penalty0 1326--1350, 2007.}
}

@misc{GW95,
  note = {M.~Goemans and D.~Williamson. \newblock Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. \newblock \emph{Journal of the ACM}, 42\penalty0 (6):\penalty0 1115--1145, 1995.}
}

@misc{GK95,
  note = {M.~D. Grigoriadis and L.~G. Khachiyan. \newblock A sublinear-time randomized approximation algorithm for matrix games. \newblock \emph{Operations Research Letters}, 18:\penalty0 53--58, 1995.}
}

@misc{Gru60,
  note = {B.~Gr{\"u}nbaum. \newblock Partitions of mass-distributions and of convex bodies by hyperplanes. \newblock \emph{Pacific J. Math}, 10\penalty0 (4):\penalty0 1257--1261, 1960.}
}

@misc{HTF01,
  note = {T.~Hastie, R.~Tibshirani, and J.~Friedman. \newblock \emph{The Elements of Statistical Learning}. \newblock Springer, 2001.}
}

@misc{Haz11,
  note = {E.~Hazan. \newblock The convex optimization approach to regret minimization. \newblock In S.~Sra, S.~Nowozin, and S.~Wright, editors, \emph{Optimization for Machine Learning}, pages 287--303. MIT press, 2011.}
}

@misc{Jag13,
  note = {M.~Jaggi. \newblock Revisiting frank-wolfe: Projection-free sparse convex optimization. \newblock In \emph{Proceedings of the 30th International Conference on Machine Learning (ICML)}, pages 427--435, 2013.}
}

@misc{JNS13,
  note = {P.~Jain, P.~Netrapalli, and S.~Sanghavi. \newblock Low-rank matrix completion using alternating minimization. \newblock In \emph{Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing}, STOC '13, pages 665--674, 2013.}
}

@misc{JZ13,
  note = {R.~Johnson and T.~Zhang. \newblock Accelerating stochastic gradient descent using predictive variance reduction. \newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}, 2013.}
}

@misc{Jon92,
  note = {L.~K. Jones. \newblock A simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training. \newblock \emph{Annals of Statistics}, pages 608--613, 1992.}
}

@misc{JN11a,
  note = {A.~Juditsky and A.~Nemirovski. \newblock First-order methods for nonsmooth convex large-scale optimization, i: General purpose methods. \newblock In S.~Sra, S.~Nowozin, and S.~Wright, editors, \emph{Optimization for Machine Learning}, pages 121--147. MIT press, 2011{\natexlab{a}}.}
}

@misc{JN11b,
  note = {A.~Juditsky and A.~Nemirovski. \newblock First-order methods for nonsmooth convex large-scale optimization, ii: Utilizing problem's structure. \newblock In S.~Sra, S.~Nowozin, and S.~Wright, editors, \emph{Optimization for Machine Learning}, pages 149--183. MIT press, 2011{\natexlab{b}}.}
}

@misc{Kar84,
  note = {N.~Karmarkar. \newblock A new polynomial-time algorithm for linear programming. \newblock \emph{Combinatorica}, 4:\penalty0 373--395, 1984.}
}

@misc{LJSB12,
  note = {S.~Lacoste-Julien, M.~Schmidt, and F.~Bach. \newblock A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method. \newblock \emph{arXiv preprint arXiv:1212.2002}, 2012.}
}

@misc{LRSB12,
  note = {N.~{Le Roux}, M.~Schmidt, and F.~Bach. \newblock A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets. \newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}, 2012.}
}

@misc{LS13,
  note = {Y.-T. Lee and A.~Sidford. \newblock Path finding i :solving linear programs with Õ(sqrt(rank)) linear system solves. \newblock \emph{Arxiv preprint arXiv:1312.6677}, 2013.}
}

@misc{LSW15,
  note = {Y.-T. Lee, A.~Sidford, and S.~C.-W Wong. \newblock A faster cutting plane method and its implications for combinatorial and convex optimization. \newblock \emph{abs/1508.04874}, 2015.}
}

@misc{Lev65,
  note = {A.~Levin. \newblock On an algorithm for the minimization of convex functions. \newblock In \emph{Soviet Mathematics Doklady}, volume 160, pages 1244--1247, 1965.}
}

@misc{Lov98,
  note = {L.~Lov\'asz. \newblock Hit-and-run mixes fast. \newblock \emph{Math. Prog.}, 86:\penalty0 443--461, 1998.}
}

@misc{Lug10,
  note = {G.~Lugosi. \newblock Comment on: $\ell_1$-penalization for mixture regression models. \newblock \emph{Test}, 19\penalty0 (2):\penalty0 259--263, 2010.}
}

@misc{MP89,
  note = {N.~Maculan and G.~G. de~Paula. \newblock A linear-time median-finding algorithm for projecting a vector on the simplex of rn. \newblock \emph{Operations research letters}, 8\penalty0 (4):\penalty0 219--222, 1989.}
}

@misc{Nem82,
  note = {A.~Nemirovski. \newblock Orth-method for smooth convex optimization. \newblock \emph{Izvestia AN SSSR, Ser. Tekhnicheskaya Kibernetika}, 2, 1982.}
}

@misc{Nem95,
  note = {A.~Nemirovski. \newblock Information-based complexity of convex programming. \newblock \emph{Lecture Notes}, 1995.}
}

@misc{Nem04,
  note = {A.~Nemirovski. \newblock Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. \newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0 229--251, 2004{\natexlab{a}}.}
}

@misc{Nem04b,
  note = {A.~Nemirovski. \newblock Interior point polynomial time methods in convex programming. \newblock \emph{Lecture Notes}, 2004{\natexlab{b}}.}
}

@misc{NY83,
  note = {A.~Nemirovski and D.~Yudin. \newblock \emph{Problem Complexity and Method Efficiency in Optimization}. \newblock Wiley Interscience, 1983.}
}

@misc{Nes83,
  note = {Y.~Nesterov. \newblock A method of solving a convex programming problem with convergence rate o($1/k^2$). \newblock \emph{Soviet Mathematics Doklady}, 27\penalty0 (2):\penalty0 372--376, 1983.}
}

@misc{Nes97,
  note = {Y.~Nesterov. \newblock Quality of semidefinite relaxation for nonconvex quadratic optimization. \newblock CORE Discussion Papers 1997019, Universit\'e catholique de Louvain, Center for Operations Research and Econometrics (CORE), 1997.}
}

@misc{Nes04,
  note = {Y.~Nesterov. \newblock \emph{Introductory lectures on convex optimization: A basic course}. \newblock Kluwer Academic Publishers, 2004{\natexlab{a}}.}
}

@misc{Nes04b,
  note = {Y.~Nesterov. \newblock Smooth minimization of non-smooth functions. \newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152, 2004{\natexlab{b}}.}
}

@misc{Nes07,
  note = {Y.~Nesterov. \newblock Gradient methods for minimizing composite objective function. \newblock Core discussion papers, Universit{\'e} catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2007.}
}

@misc{Nes12,
  note = {Y.~Nesterov. \newblock Efficiency of coordinate descent methods on huge-scale optimization problems. \newblock \emph{SIAM Journal on Optimization}, 22:\penalty0 341--362, 2012.}
}

@misc{NN94,
  note = {Y.~Nesterov and A.~Nemirovski. \newblock \emph{Interior-point polynomial algorithms in convex programming}. \newblock Society for Industrial and Applied Mathematics (SIAM), 1994.}
}

@misc{New65,
  note = {D.~Newman. \newblock Location of the maximum on unimodal surfaces. \newblock \emph{Journal of the ACM}, 12\penalty0 (3):\penalty0 395--398, 1965.}
}

@misc{NW06,
  note = {J.~Nocedal and S.~J. Wright. \newblock \emph{Numerical Optimization}. \newblock Springer, 2006.}
}

@misc{PB13,
  note = {N.~Parikh and S.~Boyd. \newblock Proximal algorithms. \newblock \emph{Foundations and Trends{\textregistered} in Optimization}, 1\penalty0 (3):\penalty0 123--231, 2013.}
}

@misc{Rak09,
  note = {A.~Rakhlin. \newblock Lecture notes on online learning. \newblock 2009.}
}

@misc{Ren01,
  note = {J.~Renegar. \newblock \emph{A mathematical view of interior-point methods in convex optimization}, volume~3. \newblock Siam, 2001.}
}

@misc{RT12,
  note = {P.~Richt\'arik and M.~Tak\'ac. \newblock Parallel coordinate descent methods for big data optimization. \newblock \emph{Arxiv preprint arXiv:1212.0873}, 2012.}
}

@misc{RM51,
  note = {H.~Robbins and S.~Monro. \newblock A stochastic approximation method. \newblock \emph{Annals of Mathematical Statistics}, 22:\penalty0 400--407, 1951.}
}

@misc{Roc70,
  note = {R.~Rockafellar. \newblock \emph{Convex Analysis}. \newblock Princeton University Press, 1970.}
}

@misc{Rud99,
  note = {M.~Rudelson. \newblock Random vectors in the isotropic position. \newblock \emph{Journal of Functional Analysis}, 164:\penalty0 60--72, 1999.}
}

@misc{SLRB11,
  note = {M.~Schmidt, N.~Le~Roux, and F.~Bach. \newblock Convergence rates of inexact proximal-gradient methods for convex optimization. \newblock In \emph{Advances in neural information processing systems}, pages 1458--1466, 2011.}
}

@misc{SS02,
  note = {B.~Sch{\"o}lkopf and A.~Smola. \newblock \emph{Learning with kernels}. \newblock MIT Press, 2002.}
}

@misc{SSS14,
  note = {S.~Shalev-Shwartz and S.~Ben-David. \newblock \emph{Understanding Machine Learning: From Theory to Algorithms}. \newblock Cambridge University Press, 2014.}
}

@misc{SSZ13,
  note = {S.~Shalev-Shwartz and T.~Zhang. \newblock Stochastic dual coordinate ascent methods for regularized loss minimization. \newblock \emph{Journal of Machine Learning Research}, 14:\penalty0 567--599, 2013{\natexlab{a}}.}
}

@misc{SSZ13b,
  note = {S.~Shalev-Shwartz and T.~Zhang. \newblock Accelerated mini-batch stochastic dual coordinate ascent. \newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}, 2013{\natexlab{b}}.}
}

@misc{SBC14,
  note = {W.~Su, S.~Boyd, and E.~Cand{\`e}s. \newblock A differential equation for modeling nesterov's accelerated gradient method: Theory and insights. \newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}, 2014.}
}

@misc{Tib96,
  note = {R.~Tibshirani. \newblock Regression shrinkage and selection via the lasso. \newblock \emph{Journal of the Royal Statistical Society. Series B (Methodological)}, 58\penalty0 (1):\penalty0 pp. 267--288, 1996.}
}

@misc{Tse08,
  note = {P.~Tseng. \newblock On accelerated proximal gradient methods for convex-concave optimization. \newblock 2008.}
}

@misc{Tsy03,
  note = {A.~Tsybakov. \newblock Optimal rates of aggregation. \newblock In \emph{Conference on Learning Theory (COLT)}, pages 303--313. 2003.}
}

@misc{Vai89,
  note = {P.~M. Vaidya. \newblock A new algorithm for minimizing convex functions over convex sets. \newblock In \emph{Foundations of Computer Science, 1989., 30th Annual Symposium on}, pages 338--343, 1989.}
}

@misc{Vai96,
  note = {P.~M. Vaidya. \newblock A new algorithm for minimizing convex functions over convex sets. \newblock \emph{Mathematical programming}, 73\penalty0 (3):\penalty0 291--341, 1996.}
}

@misc{WNF09,
  note = {S.~J. Wright, R.~D. Nowak, and M.~A.~T. Figueiredo. \newblock Sparse reconstruction by separable approximation. \newblock \emph{IEEE Transactions on Signal Processing}, 57\penalty0 (7):\penalty0 2479--2493, 2009.}
}

@misc{Xia10,
  note = {L.~Xiao. \newblock Dual averaging methods for regularized stochastic learning and online optimization. \newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 2543--2596, 2010.}
}

@misc{ZX14,
  note = {Y.~Zhang and L.~Xiao. \newblock Stochastic primal-dual coordinate method for regularized empirical risk minimization. \newblock \emph{Arxiv preprint arXiv:1409.3257}, 2014.}
}

